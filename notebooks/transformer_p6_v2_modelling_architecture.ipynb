{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cd3e3f",
   "metadata": {},
   "source": [
    "# Building Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8677c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65315e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7d170",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd418dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"aubmindlab/bert-large-arabertv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a498dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 64000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144de96",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02485f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"arbml/ArSAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996f8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus: List[str] = pd.DataFrame(dataset[\"train\"])[\"Tweet_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e26836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19897"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cdcc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text_tokens = tokenizer(text_corpus, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a591f92",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac9c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    batch_size = 10\n",
    "    hidden_size = 768\n",
    "    n_heads = 12\n",
    "    n_layers = 4\n",
    "    max_seq_len = 128\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    base = 10_000\n",
    "    bias = True\n",
    "    dropout = 0.01\n",
    "    dropout = 0.01\n",
    "    pad_id = 0\n",
    "    cls_token = 33\n",
    "    sep_token = 34\n",
    "    ignored_index = -100\n",
    "    criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f34ff",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc545717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, corpus: BatchEncoding):\n",
    "        self.corpus_tokens = corpus[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ids = torch.tensor(self.corpus_tokens[idx])\n",
    "        \n",
    "        if len(ids) < 2:\n",
    "            ids = [config.cls_token, config.sep_token]\n",
    "\n",
    "        x = ids[:-1]\n",
    "        y = ids[1:]\n",
    "        return {\"input_ids\": x, \"labels\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831f43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict[str, torch.Tensor]]):\n",
    "    \"\"\"Takes list of items of the dataset\n",
    "\n",
    "        i.e:\n",
    "    >>> ds = NanoDataset(tokenized_corpus)\n",
    "    >>> [ds[i] for i in ds]\n",
    "    \"\"\"\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=config.pad_id)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=config.ignored_index)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": (input_ids != config.pad_id).long(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee77c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NanoDataset(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccf0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=ds,\n",
    "    batch_size=config.batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5396c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 76]) torch.Size([10, 76]) torch.Size([10, 76])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape, batch[\"attention_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f3019",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef125f",
   "metadata": {},
   "source": [
    "### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9734b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSAttention(nn.Module):\n",
    "    # Casual Self Attention Module\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Softmax((Q @ KT)/sqrt(d) + M) V\n",
    "        self.qkv = ...\n",
    "        self.proj = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X: B T D\n",
    "        q, k, v = ...\n",
    "        out = ...\n",
    "        # out: B T D\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8114e1f",
   "metadata": {},
   "source": [
    "### MLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Linear Layer D x 4D \n",
    "        # + GeLU \n",
    "        # + Linear Layer 4D x D \n",
    "        # + Dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X: B T D\n",
    "        \n",
    "        # X: B T D\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6ea53",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # LayerNorm \n",
    "        # + Attention \n",
    "        # + LayerNorm \n",
    "        # + MLP \n",
    "        # + Residul\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # X: B T D\n",
    "        \n",
    "        # X: B T D\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7d236",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa32afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # transformer -> dict\n",
    "        #   - wte: word-embedding (V, D)\n",
    "        #   - wpe: pos-embedding (T, D)\n",
    "        #   - drp: Dropout\n",
    "        #   - h: -> list\n",
    "        #       -  Blocks x n_layers\n",
    "        #   - ln_f: LayerNorm\n",
    "        # lm_head: D x V (tied)\n",
    "        #\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X: B T D\n",
    "\n",
    "        # X: B T D\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float,\n",
    "        top_k: int,\n",
    "        eos: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_ids: B x T\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Generate Logits\n",
    "            logits = self.forward(input_ids, attention_mask) \n",
    "            # B T V\n",
    "            last_token_logits = ... # B 1 V\n",
    "            # Pick the top_k tokens with highest prob\n",
    "            if top_k:\n",
    "                ...\n",
    "            # Sampling\n",
    "            if temperature > 0:\n",
    "                ...\n",
    "            \n",
    "            props = ...\n",
    "            last_token = ...\n",
    "\n",
    "            # Next Step preparation\n",
    "            input_ids = ...\n",
    "            attention_mask = ...\n",
    "\n",
    "            if eos is not None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc1c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
